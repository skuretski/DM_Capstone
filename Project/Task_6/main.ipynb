{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "def get_review_df():\n",
    "  stemmer = PorterStemmer()\n",
    "  corpus = []\n",
    "  with open('../Task_6/data/hygiene.dat', 'r') as f:\n",
    "    for line in f:\n",
    "      line = re.sub(r'[^A-Za-z\\s]', '', line)\n",
    "      corpus.append(line)\n",
    "  stemmed_corpus = [stemmer.stem(word) for word in corpus]\n",
    "  # vectorizer = CountVectorizer(strip_accents=\"unicode\", stop_words=\"english\", min_df=5, max_features=100000)\n",
    "  # X = vectorizer.fit_transform(stemmed_corpus)\n",
    "\n",
    "  df = pd.DataFrame({'Stemmed_Reviews': stemmed_corpus})\n",
    "  return df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "def get_y():\n",
    "  labels = []\n",
    "  with open('../Task_6/data/hygiene.dat.labels.txt', 'r') as f:\n",
    "    for line in f:\n",
    "      line = list(line)\n",
    "      for ch in line:\n",
    "        if ch == '0' or ch == '1':\n",
    "          labels.append(int(ch))\n",
    "  f.close()\n",
    "  return labels"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "## Getting data and labels\n",
    "\n",
    "df_reviews = get_review_df()\n",
    "df_additional = pd.read_csv('../Task_6/data/hygiene.dat.additional', \n",
    "  sep=\",\", \n",
    "  usecols=[0,1,2,3], \n",
    "  names=['Categories', 'Zip_Code', 'Num_Reviews', 'Rating'],\n",
    "  dtype={'Categories': str, 'Zip_Code': str, 'Num_Reviews': str, 'Rating': float}\n",
    "  )\n",
    "df_additional['Rating'] = df_additional['Rating'].round().astype(int)\n",
    "df = df_additional.join(df_reviews)\n",
    "\n",
    "labels = get_y()\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, train_size=0.04098052, shuffle=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# Taken from https://stackoverflow.com/questions/51695322/compare-multiple-algorithms-with-sklearn-pipeline\n",
    "class ClfSwitcher(BaseEstimator):\n",
    "  def __init__(self, estimator = SGDClassifier()):\n",
    "    self.estimator = estimator\n",
    "  \n",
    "  def fit(self, X, y=None, **kwargs):\n",
    "    self.estimator.fit(X,y)\n",
    "    return self\n",
    "  \n",
    "  def predict(self, X, y=None):\n",
    "    return self.estimator.predict(X)\n",
    "    \n",
    "  def predict_proba(self, X):\n",
    "    return self.estimator.predict_proba(X)\n",
    "\n",
    "  def score(self, X, y):\n",
    "    return self.estimator.score(X, y)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "pipeline = Pipeline([\n",
    "  ('preprocessor', ColumnTransformer(\n",
    "    [\n",
    "      ('cv1', CountVectorizer(), 'Categories'),\n",
    "      ('cv2', CountVectorizer(), 'Zip_Code'),\n",
    "      ('cv3', CountVectorizer(), 'Num_Reviews'),\n",
    "      ('onehot', OneHotEncoder(dtype='int', categories='auto', sparse=False), ['Rating']),\n",
    "      ('tfidf', TfidfVectorizer(strip_accents='unicode', max_features=100000, stop_words='english'), 'Stemmed_Reviews')\n",
    "    ],\n",
    "    remainder= 'passthrough')\n",
    "  ),\n",
    "  ('clf', ClfSwitcher())\n",
    "])\n",
    "\n",
    "parameters = [\n",
    "  {\n",
    "    'clf__estimator': [SGDClassifier()], # SVM if hinge loss / logreg if log loss\n",
    "    'preprocessor__tfidf__max_df': (0.5, 0.75, 0.9),\n",
    "    'preprocessor__tfidf__min_df': (1,10,20),\n",
    "    'clf__estimator__penalty': ('l2', 'elasticnet', 'l1'),\n",
    "    #'clf__estimator__max_iter': [50, 80],\n",
    "    'clf__estimator__tol': [1e-4],\n",
    "    'clf__estimator__loss': ['hinge', 'log', 'modified_huber'],\n",
    "},\n",
    "{\n",
    "    'clf__estimator': [MultinomialNB()],\n",
    "    'preprocessor__tfidf__max_df': (0.5, 0.75, 0.9),\n",
    "    'preprocessor__tfidf__min_df': (1,10,20),\n",
    "    'clf__estimator__alpha': (1e-1, 1e-2, 1e-3),\n",
    "  },\n",
    "]\n",
    "gscv = GridSearchCV(pipeline, parameters, cv=3, verbose=1, scoring='f1_macro')\n",
    "gscv.fit(X_train, y_train)\n",
    "results = gscv.cv_results_\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "  'Rank': results['rank_test_score'], \n",
    "  'Mean F1 Score': results['mean_test_score'], \n",
    "  'Params': results['params'], \n",
    "  'Mean Fit Time': results['mean_fit_time']\n",
    "  }).sort_values(by=['Rank'])\n",
    "results_df.to_html('results.html')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('capstone': conda)"
  },
  "interpreter": {
   "hash": "1f88b0d4213c0275882fbc80e2a78b1740d740e0202ec2bdf9eae60d3775edd8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}